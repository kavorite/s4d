This repository contains a minimal [haiku] implementation of **S**equence **S**tructured **S**tate **S**pace models ([S4]), specifically a variant that uses a diagonal state matrix [S4D], which "recovers S4's performance on most tasks." These modules are useful for sequence transduction, competitive in language-modeling tasks when replacing self-attention in an architecture such as an encoder-only transformer stack, but their novel parametrization makes them more compact compared to attention-based architectures, as the space required for adequate intermediate representational capacity scales _linearly,_ rather than quadratically, with respect to the sequence length they expect to encounter. This opens up entirely new and previously-untapped application domains, for which applying transformer architecture design was previously intractableâ€” such as [end-to-end conditional music generation on raw audio samples][sashimi].

These models take a principled approach to long-range dependencies, and are theoretically capable of representing such dependencies over arbitary sequence lengths. Empirically they perform well for sequence lengths of up to several thousand steps on a variety of discrete timescales. They also operate an order of magnitude faster than their attention-based counterparts during autoregressive inference in a fraction of the space required for a forward pass over an entire sequence, which makes them much cheaper at inference-time for generative tasks. See [s4dbert.py](examples/s4dbert.py) for a worked example of how to instantiate a simple S4 transformer using the API.

[s4]: https://github.com/HazyResearch/state-spaces
[s4d]: https://github.com/HazyResearch/state-spaces/blob/6cbc09aeeebfe72b7bde7897ef157cf63fd12721/src/models/sequence/ss/standalone/s4d.py
[haiku]: https://github.com/deepmind/haiku
[sashimi]: https://arxiv.org/abs/2202.09729